# Main configuration file for Hydra.
# This is the entry point for all configuration management.
# Specific configs are organized in subdirectories.

# Defaults - these are applied in order
defaults:
  - override hydra/hydra_logging: none
  - override hydra/job_logging: none

# Model configuration
model:
  name: "convnext_tiny"  # Using base ConvNext (no CBAM) for faster training on M2
  num_classes: 2  # Ball Screw Drive: binary classification (defect/no_defect)
  pretrained: true  # Use pretrained - much faster convergence
  cbam_stages: [3, 4]  # Ignored for convnext_tiny

# Loss function configuration
loss:
  type: "cross_entropy"  # Binary classification for Ball Screw Drive dataset
  weight: null  # Set to null for uniform weighting (balanced dataset)
  reduction: "mean"

# Data configuration
data:
  img_dir: "data/images"
  ann_dir: "data/annotations"  # Not used for Ball Screw Drive dataset
  splits_dir: "data/splits"  # Directory for saved train/val/test splits
  batch_size: 64  # Increased from 32 - M2 can handle this with 150x150 images
  num_workers: 0  # 0 for MPS to avoid multiprocessing overhead
  pin_memory: false  # Not supported on MPS
  image_size: [150, 150]  # Ball Screw Drive dataset: 150x150 images
  num_classes: 2  # Ball Screw Drive: binary classification
  class_names: ["no_defect", "defect"]
  
  # Split strategy: use pre-saved splits from setup script
  split_strategy: "stratified_70_15_15"
  use_saved_splits: true  # Use pre-saved splits from data/splits/

# Augmentation configuration
augmentation:
  horizontal_flip: 0.5
  vertical_flip: 0.3  # Reduced from 0.5 - less augmentation = faster
  rotation: 10  # Reduced from 15 - less rotation = faster
  brightness_contrast:
    brightness: 0.2  # Reduced from 0.3 - less augmentation = faster
    contrast: 0.2
  
  # Defect Blackout Augmentation (disabled for NEU dataset)
  defect_blackout:
    enabled: false

# Training configuration
training:
  num_epochs: 50  # Reduced from 100 - balanced dataset converges faster
  learning_rate: 0.001
  weight_decay: 0.001  # Increased from 0.0001 - stronger L2 regularization
  warmup_epochs: 2  # Reduced from 3 - faster warmup
  early_stopping_patience: 5  # Reduced from 8 - stop earlier
  early_stopping_metric: "f1_macro"
  log_interval: 5  # More frequent logging to see progress
  threshold: 0.5
  save_checkpoints: false  # Disable checkpoint artifacts to save disk space
  
  # Optional: freeze backbone for transfer learning
  freeze_backbone: true  # Freeze pretrained weights - only train classifier
  freeze_early_stages: null  # Set to number of stages to freeze

# Optimization
optimizer:
  type: "adamw"
  lr: 0.001  # Standard learning rate
  weight_decay: 0.0001
  betas: [0.9, 0.999]

scheduler:
  type: "cosine"
  warmup_epochs: 3

# Experiment configuration
experiment:
  name: null  # Will be generated from timestamp if null
  seed: 42 # TODO: Change seed for different runs
  save_dir: "code/experiments/results"
  output_dir: null  # Populated at runtime for downstream scripts

# Hydra specific config
hydra:
  run:
    # Use a simpler output directory structure that prioritizes the experiment name.
    # This makes it easier for aggregation scripts to find the results.
    dir: code/experiments/results/${experiment.name}_${now:%Y%m%d_%H%M%S}
  job:
    chdir: false
