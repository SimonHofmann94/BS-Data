# Main configuration file for Hydra.
# This is the entry point for all configuration management.
# Specific configs are organized in subdirectories.

# Defaults - these are applied in order
defaults:
  - override hydra/hydra_logging: none
  - override hydra/job_logging: none

# Model configuration
model:
  name: "convnext_tiny"  # Using base ConvNext (no CBAM) for faster training on M2
  num_classes: 6  # NEU dataset: 6 defect types
  pretrained: true  # Use pretrained - much faster convergence
  cbam_stages: [3, 4]  # Ignored for convnext_tiny

# Loss function configuration
loss:
  type: "cross_entropy"  # Single-label classification for NEU dataset
  weight: null  # Set to null for uniform weighting (balanced dataset)
  reduction: "mean"

# Data configuration
data:
  img_dir: "data/images"
  ann_dir: "data/annotations"  # Not used for NEU dataset
  splits_dir: "data/splits"  # Directory for saved train/val/test splits
  batch_size: 32  # Balanced for M2 8GB
  num_workers: 0  # 0 for MPS to avoid multiprocessing overhead
  pin_memory: false  # Not supported on MPS
  image_size: [200, 200]  # NEU dataset: 200x200 images
  num_classes: 6  # NEU: 6 defect types
  class_names: ["crazing", "inclusion", "patches", "pitted_surface", "rolled-in_scale", "scratches"]
  
  # Split strategy: use pre-saved splits from setup script
  split_strategy: "stratified_70_15_15"
  use_saved_splits: true  # Use pre-saved splits from data/splits/

# Augmentation configuration
augmentation:
  horizontal_flip: 0.5
  vertical_flip: 0.5
  rotation: 15  # Enable rotation to increase variety
  brightness_contrast:
    brightness: 0.3  # Increased augmentation strength
    contrast: 0.3
  
  # Defect Blackout Augmentation (disabled for NEU dataset)
  defect_blackout:
    enabled: false

# Training configuration
training:
  num_epochs: 50  # Reduced from 100 - balanced dataset converges faster
  learning_rate: 0.001
  weight_decay: 0.001  # Increased from 0.0001 - stronger L2 regularization
  warmup_epochs: 3  # Reduced from 5
  early_stopping_patience: 8  # Reduced from 10 - stop sooner if overfitting
  early_stopping_metric: "f1_macro"
  log_interval: 5  # More frequent logging to see progress
  threshold: 0.5
  
  # Optional: freeze backbone for transfer learning
  freeze_backbone: true  # Freeze pretrained weights - only train classifier
  freeze_early_stages: null  # Set to number of stages to freeze

# Optimization
optimizer:
  type: "adamw"
  lr: 0.001  # Standard learning rate
  weight_decay: 0.0001
  betas: [0.9, 0.999]

scheduler:
  type: "cosine"
  warmup_epochs: 3

# Experiment configuration
experiment:
  name: null  # Will be generated from timestamp if null
  seed: 42
  save_dir: "code/experiments/results"

# Hydra specific config
hydra:
  run:
    dir: code/experiments/results/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: false
